{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5b7fbd-a440-43fd-bd92-891023750fa4",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca969e7c-3017-4a30-bec4-e4dc51897195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (2021.9.30)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.11.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/miniconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /Users/anshulkaushal/.local/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/miniconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: sklearn in /opt/miniconda3/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/anshulkaushal/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.9/site-packages (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/lib/python3.9/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.9/site-packages (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "!pip install sklearn\n",
    "!pip install torch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe92b26-9ea6-4802-ad8f-27ed3b5cadcd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5658c92b-06f9-4987-be0b-42f5406e8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from transformers.models.bert.modeling_bert import BertForTokenClassification\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a43742-1a7c-43b7-a4ae-90875606d41a",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1432ac4-99be-4e8d-acc7-5e37e7217c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_word_ids(texts):\n",
    "    '''\n",
    "    str -> list\n",
    "    Returns label_ids corresponding to the tokens in the sentence\n",
    "    \n",
    "    Params:\n",
    "        texts (str) the sentence\n",
    "    \n",
    "    Returns:\n",
    "        label_ids (list) a list of label_ids\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=MAX_LENGTH)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "def predict(model, sentence):\n",
    "    '''\n",
    "    model, str -> list\n",
    "    returns the tagged list corresponding to the tokenized\n",
    "    sentence\n",
    "    \n",
    "    Params:\n",
    "        model (torch.nn.Module) the fine-tuned tagger trained on\n",
    "        manually annotated comments from espncricinfo\n",
    "        sentence (str) a single comment for which the tags are to be\n",
    "        predicted\n",
    "    \n",
    "    Returns:\n",
    "        prediction_label (list) IOB tagged list \n",
    "    '''\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n",
    "\n",
    "    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids2tags[i] for i in predictions]\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1687c337-2676-45fe-876e-66a5ba56c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(comm):\n",
    "    '''\n",
    "    str -> list\n",
    "    Returns tokens after removing special tokens and attention masks\n",
    "    \n",
    "    Params:\n",
    "        comm (str) comment for which the tokens are required\n",
    "    \n",
    "    Returns:\n",
    "        tokens (list) list of tokens\n",
    "    '''\n",
    "    token_list = tokenizer.convert_ids_to_tokens(tokenizer.encode(comm, padding='max_length', max_length=MAX_LENGTH))\n",
    "    tokens = []\n",
    "    pattern = re.compile(r'^#.*')\n",
    "    while True:\n",
    "        for ind, token in enumerate(token_list):\n",
    "            if token == '[CLS]' or token == '[SEP]' or token == '[PAD]':\n",
    "                continue\n",
    "            else:\n",
    "                if (not ind == (len(token_list) - 1)) and ('#' in token_list[ind+1]):\n",
    "                    tokens.append(joiner(token, token_list[ind+1].lstrip('#')))\n",
    "                else:\n",
    "                    if '#' in token:\n",
    "                        continue\n",
    "                    else:\n",
    "                        tokens.append(token)\n",
    "        if any((match := pattern.match(item)) for item in tokens):\n",
    "            token_list = tokens.copy()\n",
    "            tokens.clear()\n",
    "        else:\n",
    "            break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4329cf-75a6-48f8-b2d9-7d0e3aa6d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(tag_list, comm):\n",
    "    '''\n",
    "    tag_list, comm -> str/None\n",
    "    returns the line of the bowl mentioned in the comment\n",
    "    \n",
    "    Params:\n",
    "        tag_list (list) the tagged representation of comment \n",
    "        comm (str) the comment\n",
    "    \n",
    "    Returns:\n",
    "        line (str) If there is a line tagged\n",
    "        None (None) If there is no line tagged\n",
    "    '''\n",
    "    tokens = get_tokens(comm)\n",
    "    assert len(tokens) == len(tag_list)\n",
    "    line = ''\n",
    "    for idx, tag in enumerate(tag_list):\n",
    "        if tag == 'B-LINE' or tag == 'I-LINE':\n",
    "            line += tokens[idx]\n",
    "            for i in range(idx+1, len(tag_list)):\n",
    "                if tag_list[i] == 'I-LINE' or tag_list[i] == 'B-LINE':\n",
    "                    line += ' ' + tokens[i]\n",
    "            return line\n",
    "        else:\n",
    "            continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7557b066-8ea4-4f87-a82f-139eff3ed7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(tag_list, comm):\n",
    "    '''\n",
    "    tag_list, comm -> str/None\n",
    "    returns the length of the bowl mentioned in the comment\n",
    "    \n",
    "    Params:\n",
    "        tag_list (list) the tagged representation of comment \n",
    "        comm (str) the comment\n",
    "    \n",
    "    Returns:\n",
    "        line (str) If there is a length tagged\n",
    "        None (None) If there is no length tagged\n",
    "    '''\n",
    "    tokens = get_tokens(comm)\n",
    "    assert len(tokens) == len(tag_list)\n",
    "    length = ''\n",
    "    for idx, tag in enumerate(tag_list):\n",
    "        if tag == 'B-LENGTH' or tag == 'I-LENGTH':\n",
    "            length += tokens[idx]\n",
    "            for i in range(idx+1, len(tag_list)):\n",
    "                if tag_list[i] == 'I-LENGTH' or tag_list[i] == 'B-LENGTH':\n",
    "                    length += ' ' + tokens[i]\n",
    "            return length\n",
    "        else:\n",
    "            continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fbe2290-f132-4afa-9d44-a0b076a3a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joiner(this, next_this):\n",
    "    '''\n",
    "    removes attention mask and joins tokens\n",
    "    \n",
    "    Params:\n",
    "        this (str) first token\n",
    "        next_this (str) second token\n",
    "        \n",
    "    Returns:\n",
    "        this + next_this (str) joined tokens \n",
    "    '''\n",
    "    for i, char in enumerate(next_this[::-1]):\n",
    "        if char == this[-1]:\n",
    "            if char == next_this[0]:\n",
    "                return this.rstrip(char) + next_this\n",
    "            elif this[-2] == next_this[::-1][i+1]:\n",
    "                return this.rstrip(next_this[::-1][i:][::-1]) + next_this\n",
    "            else:\n",
    "                return this + next_this\n",
    "\n",
    "    return this + next_this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cffcb3-f29a-4dd1-8fb8-7a01b9e5b44a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8619b8e0-fa13-48a7-9fd8-c12fbd30bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(model_dir, num_labels=len(unique_tags))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a5dd8-e1f3-4216-a194-a02a5d5c9370",
   "metadata": {},
   "source": [
    "## Loading intermediate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c79e1a-6b23-4408-8a76-57929b8685f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_files/unique_tags.pkl', 'rb') as handle:\n",
    "    unique_tags = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99527c88-89d4-404d-b790-404a1b048fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_files/ids2tags.pkl', 'rb') as handle:\n",
    "    ids2tags = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffff56e8-7533-45fc-8684-a4c2656383e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_files/tags2ids.pkl', 'rb') as handle:\n",
    "    tags2ids = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b478baf-87e7-4070-8c04-b6cb7c6a26e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermediate_files/max_length.pkl', 'rb') as handle:\n",
    "    MAX_LENGTH = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddf86b-b238-4a6c-b4e5-4cd673adf109",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04c03f4-7869-4032-9814-6481dbecf5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at models/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = 'models/'\n",
    "model_path = 'models/tagger.pt'\n",
    "model = BertModel()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee27da-6e53-410d-88dd-f5f7b20f9017",
   "metadata": {},
   "source": [
    "## Loading pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb3e6ce-8533-470f-b2e8-431a88f2fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781ad5a-0a0d-42b8-b9e0-6b4e862d1581",
   "metadata": {},
   "source": [
    "## Model demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaec45-f8f0-4656-958f-908bbb1f7d16",
   "metadata": {},
   "source": [
    "### Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6f6684-4d54-452b-bfab-47de0447055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = \"no-nonsense Short of a length at middle stump front leg in the leg side and an old fashioned slog across the line one bounce over the long-on boundary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f3690-fbd8-4bcf-80e5-1aed89f7148e",
   "metadata": {},
   "source": [
    "### Model (tagger) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47fb658f-7fa3-4a3c-a06a-7726b5242041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LENGTH',\n",
       " 'I-LENGTH',\n",
       " 'I-LENGTH',\n",
       " 'I-LENGTH',\n",
       " 'O',\n",
       " 'B-LINE',\n",
       " 'I-LINE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787d359-c00c-4749-be8a-300d15208547",
   "metadata": {},
   "source": [
    "### Extract line of the delivery from the comment using the tagger prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ad761e-d60d-4219-88f0-acf0a94fd4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middle stump'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_line(predict(model, comm), comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c55e1e-35ec-4ff9-b55a-484f433c0ae2",
   "metadata": {},
   "source": [
    "### Extract length of the delivery from the comment using the tagger prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a86964b-0ab3-45a9-ac36-efe0f679b742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'short of a length'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_length(predict(model, comm), comm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
